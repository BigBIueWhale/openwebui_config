docker load -i openwebui.docker

docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=http://192.168.4.22:11434 -e OFFLINE_MODE=True -v "open-webui:/C/openwebui_data/" --name open-webui --restart always ghcr.io/open-webui/open-webui:main

Then set "task model" to codestral (for generating chat titles)

From the admin panel in the GUI:
Permanently customize codestral:22b model parameters to have "context length" 30000, "num_predict" 29000, "temperature" 0.1, "Top P" 0.95, "repeat_penalty" 1.15

Permanently customize qwq:32b model parameters to have "context length": 14000 and "num_predict" 13000. We don't want to set the context length too high because then Ollama might decide to start using the CPU instead of the GPU. 

From within the admin UI: Make new user signins be "user" instead of "pending" or "admin"- and turn on new user signups.

From within the admin UI: Make all models "public" so all users can access them.

Turn of code interpreter because the models don't understand it. Use pyodied (default) for python code execution.
